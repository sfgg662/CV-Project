import os
import random
import zipfile
import urllib.request
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from torchvision.datasets.utils import download_url
import torch.nn.functional as F

# -------------------------
# åŸºæœ¬é…ç½®
# -------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMAGE_SIZE = 256
BATCH_SIZE = 4
EPOCHS = 4  # å¢åŠ è®­ç»ƒè½®æ•°
LR = 1e-3
NUM_CONTENT_IMAGES = 5000  # å¢åŠ è®­ç»ƒæ•°æ®é‡
STYLE_WEIGHT = 1e4  # é£æ ¼æŸå¤±æƒé‡ï¼ˆè°ƒæ•´ä¸ºä¸å½’ä¸€åŒ–åçš„VGGè¾“å‡ºé‡çº§åŒ¹é…ï¼‰
CONTENT_WEIGHT = 1.0  # å†…å®¹æŸå¤±æƒé‡
TV_WEIGHT = 1e-6  # Total Variation å¹³æ»‘æŸå¤±æƒé‡
COLOR_WEIGHT = 1e-2  # é¢œè‰²å®ˆæ’æŸå¤±æƒé‡ï¼ˆåŒ¹é…è¾“å‡ºä¸å†…å®¹çš„é€šé“å‡å€¼/æ–¹å·®ï¼‰
# å¤šå°ºåº¦/åˆ†å±‚é£æ ¼è¶…å‚æ•°
# VGG æå–çš„4å±‚å¯¹åº”çš„æƒé‡ï¼ˆrelu1_2, relu2_2, relu3_3, relu4_3ï¼‰
STYLE_LAYER_WEIGHTS = [0.5, 0.3, 0.1, 0.1]
# é£æ ¼å›¾ä½¿ç”¨çš„ä¸åŒåˆ†è¾¨ç‡ï¼ˆåƒç´ å¤§å°ï¼‰ï¼Œä¼šè®¡ç®—æ¯ä¸ªå°ºåº¦çš„ Gram çŸ©é˜µ
STYLE_SCALES = [IMAGE_SIZE, IMAGE_SIZE * 2]
# æ¯ä¸ªå°ºåº¦çš„æƒé‡ï¼Œé•¿åº¦éœ€ä¸ STYLE_SCALES ä¸€è‡´
STYLE_SCALE_WEIGHTS = [1.0, 0.5]

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
COCO_DIR = os.path.join(ROOT_DIR, "coco")
STYLE_PATH = os.path.join(ROOT_DIR, "style.png")
TEST_CONTENT_PATH = os.path.join(ROOT_DIR, "testcontent.png")
MODEL_PATH = os.path.join(ROOT_DIR, "realtime_style.pth")
OUTPUT_PATH = os.path.join(ROOT_DIR, "output_stylized.png")
CHECKPOINT_DIR = os.path.join(ROOT_DIR, "checkpoints")
TEST_CONTENTS_DIR = os.path.join(ROOT_DIR, "testcontents")

# -------------------------
# ä¸‹è½½ COCO val2017 (å°æ•°æ®é›†ï¼Œä»…1GB)
# -------------------------
def download_coco():
    os.makedirs(COCO_DIR, exist_ok=True)
    img_dir = os.path.join(COCO_DIR, "val2017")

    if os.path.exists(img_dir):
        num_images = len([f for f in os.listdir(img_dir) if f.endswith('.jpg')])
        print(f"âœ… COCO å·²å­˜åœ¨ ({num_images} å¼ å›¾ç‰‡)ï¼Œè·³è¿‡ä¸‹è½½")
        return img_dir

    url = "http://images.cocodataset.org/zips/val2017.zip"
    zip_path = os.path.join(COCO_DIR, "val2017.zip")

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´çš„zipæ–‡ä»¶
    if os.path.exists(zip_path):
        print("âš ï¸  æ£€æµ‹åˆ°ä¸å®Œæ•´çš„zipæ–‡ä»¶ï¼Œåˆ é™¤ä¸­...")
        os.remove(zip_path)

    try:
        print("â¬‡ï¸ ä¸‹è½½ COCO val2017 (çº¦1GBï¼ŒåŒ…å«5000å¼ å›¾ç‰‡)...")
        download_url(url, COCO_DIR, filename="val2017.zip")
        
        print("ğŸ“¦ éªŒè¯zipæ–‡ä»¶...")
        if not zipfile.is_zipfile(zip_path):
            raise Exception("ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„zipæ–‡ä»¶")
        
        print("ğŸ“¦ è§£å‹ä¸­...")
        with zipfile.ZipFile(zip_path, 'r') as z:
            z.extractall(COCO_DIR)
        
        os.remove(zip_path)
        print("âœ… COCOæ•°æ®é›†ä¸‹è½½å®Œæˆ")
        return img_dir
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        if os.path.exists(zip_path):
            os.remove(zip_path)
        raise

# -------------------------
# Dataset
# -------------------------
class CocoSubset(Dataset):
    def __init__(self, image_dir, num_images):
        all_imgs = os.listdir(image_dir)
        self.imgs = random.sample(all_imgs, num_images)

        self.transform = transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            transforms.CenterCrop(IMAGE_SIZE),
            transforms.ToTensor()
        ])
        self.image_dir = image_dir

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, idx):
        path = os.path.join(self.image_dir, self.imgs[idx])
        img = Image.open(path).convert("RGB")
        return self.transform(img)

# -------------------------
# Transformer Network
# -------------------------
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(channels, channels, 3, 1, 1),
            nn.InstanceNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, 3, 1, 1),
            nn.InstanceNorm2d(channels)
        )

    def forward(self, x):
        return x + self.block(x)

class TransformerNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 32, 9, 1, 4),
            nn.InstanceNorm2d(32),
            nn.ReLU(inplace=True),

            nn.Conv2d(32, 64, 3, 2, 1),
            nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True),

            nn.Conv2d(64, 128, 3, 2, 1),
            nn.InstanceNorm2d(128),
            nn.ReLU(inplace=True),

            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),

            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True),

            nn.Upsample(scale_factor=2),
            nn.Conv2d(64, 32, 3, 1, 1),
            nn.InstanceNorm2d(32),
            nn.ReLU(inplace=True),

            nn.Conv2d(32, 3, 9, 1, 4)
        )

    def forward(self, x):
        return self.model(x)

# -------------------------
# VGG Feature Extractor
# -------------------------
class VGG16(nn.Module):
    def __init__(self):
        super().__init__()
        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_FEATURES).features
        self.layers = nn.ModuleList(vgg[:23])
        # register imagenet normalization (expects input in [0,1])
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
        for p in self.parameters():
            p.requires_grad = False

    def forward(self, x):
        # x is expected in [0,1]; apply ImageNet normalization
        x = (x - self.mean) / self.std
        features = []
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i in {3, 8, 15, 22}:
                features.append(x)
        return features # è¿™é‡Œåªéœ€è¦æå–4ä¸ªæŒ‡å®šå±‚çš„ç‰¹å¾ ï¼ï¼ï¼

def gram_matrix(x):
    b, c, h, w = x.size()
    f = x.view(b, c, h * w)
    g = torch.bmm(f, f.transpose(1, 2))
    return g / (c * h * w)

# -------------------------
# ä¸»è®­ç»ƒæµç¨‹
# -------------------------
def main(style_name=None):
    img_dir = download_coco()
    dataset = CocoSubset(img_dir, NUM_CONTENT_IMAGES)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    transformer = TransformerNet().to(DEVICE)
    vgg = VGG16().to(DEVICE).eval()
    optimizer = optim.Adam(transformer.parameters(), LR)

    # è®¡ç®—å¤šå°ºåº¦é£æ ¼ Gram çŸ©é˜µï¼ˆæ¯ä¸ªå°ºåº¦ä¿å­˜ä¸€ç»„ layer-wise gramsï¼‰
    # ç¡®å®šä½¿ç”¨çš„é£æ ¼å›¾ï¼ˆæ”¯æŒ styles ç›®å½•ä¸‹çš„æŒ‰åé€‰æ‹©ï¼‰
    if style_name:
        # æ”¯æŒä¼ å…¥å¸¦æˆ–ä¸å¸¦æ‰©å±•åçš„ style åç§°
        if style_name.lower().endswith(('.png', '.jpg', '.jpeg')):
            candidate = os.path.join(ROOT_DIR, 'styles', style_name)
        else:
            candidate = os.path.join(ROOT_DIR, 'styles', f"{style_name}.png")
        if not os.path.exists(candidate):
            print(f"âŒ æœªæ‰¾åˆ°é£æ ¼å›¾: {candidate}")
            if os.path.exists(os.path.join(ROOT_DIR, 'styles')):
                files = [f for f in os.listdir(os.path.join(ROOT_DIR, 'styles')) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                print("å¯ç”¨çš„ stylesï¼š")
                for f in files[:50]:
                    print(f"  - {f}")
            raise SystemExit(1)
        style_img = Image.open(candidate).convert("RGB")
        style_tag = os.path.splitext(os.path.basename(candidate))[0]
    else:
        style_img = Image.open(STYLE_PATH).convert("RGB")
        style_tag = 'default'
    style_grams_scales = []
    if len(STYLE_SCALES) != len(STYLE_SCALE_WEIGHTS):
        raise ValueError("STYLE_SCALES å’Œ STYLE_SCALE_WEIGHTS é•¿åº¦éœ€ç›¸åŒ")

    for s in STYLE_SCALES:
        style_tf = transforms.Compose([
            transforms.Resize(s),
            transforms.CenterCrop(s),
            transforms.ToTensor()
        ])
        style_resized = style_tf(style_img).unsqueeze(0).to(DEVICE)
        style_feats = vgg(style_resized)
        style_grams = [gram_matrix(f) for f in style_feats]
        style_grams_scales.append(style_grams)

    # å½’ä¸€åŒ–å¹¶æ ¡éªŒå±‚æƒé‡ä¸å°ºåº¦æƒé‡ï¼Œé¿å…è´Ÿå€¼æˆ–å’Œä¸º0
    import math
    layer_ws = torch.tensor(STYLE_LAYER_WEIGHTS, dtype=torch.float32)
    if (layer_ws < 0).any():
        raise ValueError("STYLE_LAYER_WEIGHTS ä¸­ä¸èƒ½å«æœ‰è´Ÿå€¼")
    layer_sum = layer_ws.sum().item()
    if math.isclose(layer_sum, 0.0):
        raise ValueError("STYLE_LAYER_WEIGHTS çš„å’Œä¸èƒ½ä¸º0")
    normalized_layer_ws = (layer_ws / layer_sum).tolist()

    scale_ws = torch.tensor(STYLE_SCALE_WEIGHTS, dtype=torch.float32)
    if (scale_ws < 0).any():
        raise ValueError("STYLE_SCALE_WEIGHTS ä¸­ä¸èƒ½å«æœ‰è´Ÿå€¼")
    scale_sum = scale_ws.sum().item()
    if math.isclose(scale_sum, 0.0):
        raise ValueError("STYLE_SCALE_WEIGHTS çš„å’Œä¸èƒ½ä¸º0")
    normalized_scale_ws = (scale_ws / scale_sum).tolist() # ä¸ºäº†ç¡®ä¿æ€»å’Œä¸º1ï¼Œæ‰€ä»¥è¿›è¡Œä¸€ä¸ªå½’ä¸€åŒ–

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"è®­ç»ƒè½®æ•°: {EPOCHS} | å›¾ç‰‡æ•°é‡: {NUM_CONTENT_IMAGES} | æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"å†…å®¹æƒé‡: {CONTENT_WEIGHT} | é£æ ¼æƒé‡: {STYLE_WEIGHT}")
    
    for epoch in range(EPOCHS):
        epoch_loss = 0
        for i, content in enumerate(loader):
            content = content.to(DEVICE)

            stylized = transformer(content)
            content_feats = vgg(content)
            stylized_feats = vgg(stylized)

            content_loss = torch.mean((stylized_feats[1] - content_feats[1]) ** 2) # åªé€‰ç¬¬ä¸€ä¸ªç‰¹å¾å±‚

            # è®¡ç®—å¤šå°ºåº¦ã€å¤šå±‚æ¬¡åŠ æƒçš„é£æ ¼æŸå¤±
            style_loss = 0.0
            # éªŒè¯å±‚æƒé‡é•¿åº¦
            num_layers = len(stylized_feats)
            if len(STYLE_LAYER_WEIGHTS) != num_layers:
                raise ValueError(f"STYLE_LAYER_WEIGHTS é•¿åº¦åº”ä¸º {num_layers}")

            for s_idx, (scale_grams, scale_w) in enumerate(zip(style_grams_scales, normalized_scale_ws)): # è¿™é‡Œæ˜¯å½’ä¸€åŒ–ä¹‹åçš„æƒé‡
                target_size = STYLE_SCALES[s_idx]
                # æŠŠ stylized ç¼©æ”¾åˆ°å½“å‰å°ºåº¦å†æç‰¹å¾
                stylized_scaled = F.interpolate(stylized, size=target_size, mode='bilinear', align_corners=False)
                scaled_feats = vgg(stylized_scaled)
                per_scale_loss = 0.0
                for l_idx, (sf, sg) in enumerate(zip(scaled_feats, scale_grams)):
                    layer_w = normalized_layer_ws[l_idx]
                    per_scale_loss += layer_w * torch.mean((gram_matrix(sf) - sg) ** 2)
                style_loss += scale_w * per_scale_loss

            # Total Variation æŸå¤±ï¼Œå¹³æ»‘è¾“å‡º
            def total_variation_loss(img):
                # img: (B, C, H, W)
                dh = torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]).mean()
                dw = torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]).mean()
                return dh + dw

            tv_loss = total_variation_loss(stylized)

            # é¢œè‰²å®ˆæ’æŸå¤±ï¼šåŒ¹é…æ¯é€šé“çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œé˜²æ­¢å±€éƒ¨äº®æ–‘å’Œå¤±çœŸ
            # åœ¨ [0,1] èŒƒå›´ä¸Šè®¡ç®—
            def color_stats_loss(x, y):
                # x, y: (B, C, H, W)
                mx = x.mean(dim=[0, 2, 3])
                my = y.mean(dim=[0, 2, 3])
                sx = x.std(dim=[0, 2, 3])
                sy = y.std(dim=[0, 2, 3])
                return torch.mean((mx - my) ** 2) + torch.mean((sx - sy) ** 2)

            color_loss = color_stats_loss(stylized.clamp(0.0, 1.0), content)

            loss = CONTENT_WEIGHT * content_loss + STYLE_WEIGHT * style_loss + TV_WEIGHT * tv_loss + COLOR_WEIGHT * color_loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()

            if i % 50 == 0:
                print(f"Epoch {epoch+1}/{EPOCHS} | Step {i}/{len(loader)} | "
                      f"Loss: {loss.item():.2f} | Content: {content_loss.item():.2f} | "
                      f"Style: {style_loss.item():.4f}")
        
        # æ¯ä¸ªepochç»“æŸä¿å­˜æ£€æŸ¥ç‚¹å’Œæµ‹è¯•å›¾ç‰‡
        avg_loss = epoch_loss / len(loader)
        print(f"\nğŸ“Š Epoch {epoch+1} å®Œæˆ | å¹³å‡Loss: {avg_loss:.2f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å« style_tag ä»¥åŒºåˆ†ä¸åŒé£æ ¼ï¼‰
        checkpoint_path = os.path.join(CHECKPOINT_DIR, f"model_{style_tag}_epoch_{epoch+1}.pth")
        torch.save(transformer.state_dict(), checkpoint_path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # ç”Ÿæˆæµ‹è¯•å›¾ç‰‡
        if os.path.exists(TEST_CONTENT_PATH):
            transformer.eval()
            with torch.no_grad():
                test_img = Image.open(TEST_CONTENT_PATH).convert("RGB")
                test_tf = transforms.Compose([
                    transforms.Resize(IMAGE_SIZE),
                    transforms.CenterCrop(IMAGE_SIZE),
                    transforms.ToTensor()
                ])
                test_tensor = test_tf(test_img).unsqueeze(0).to(DEVICE)
                test_output = transformer(test_tensor).cpu().clamp(0.0, 1.0)
                test_result = transforms.ToPILImage()(test_output[0])
                test_result.save(os.path.join(CHECKPOINT_DIR, f"test_epoch_{epoch+1}.png"))
                print(f"ğŸ¨ æµ‹è¯•å›¾ç‰‡å·²ä¿å­˜: test_epoch_{epoch+1}.png\n")
            transformer.train()
        print("="*80)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ŒæŒ‰ style_tag å‘½åï¼ŒåŒæ—¶è¦†ç›–é»˜è®¤æ¨¡å‹æ–‡ä»¶ä»¥ä¾¿åç»­é»˜è®¤æ¨ç†
    final_model_path = os.path.join(ROOT_DIR, f"realtime_style-{style_tag}.pth")
    torch.save(transformer.state_dict(), final_model_path)
    # ä¹Ÿä¿å­˜åˆ°é»˜è®¤æ¨¡å‹è·¯å¾„ï¼Œä¾¿äºä»¥å‰çš„æ¨ç†å‘½ä»¤ç»§ç»­å·¥ä½œ
    torch.save(transformer.state_dict(), MODEL_PATH)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜: {final_model_path} (åŒæ—¶æ›´æ–° {MODEL_PATH})")

# -------------------------
# æ¨ç†å‡½æ•°ï¼šå¤„ç†å•å¼ å›¾ç‰‡
# -------------------------
def stylize_image(content_path, model_path, output_path):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œé£æ ¼è¿ç§»
    
    Args:
        content_path: å¾…å¤„ç†çš„å†…å®¹å›¾è·¯å¾„
        model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºå›¾ç‰‡ä¿å­˜è·¯å¾„
    """
    print(f"ğŸ¨ åŠ è½½æ¨¡å‹: {model_path}")
    transformer = TransformerNet().to(DEVICE)
    transformer.load_state_dict(torch.load(model_path, map_location=DEVICE))
    transformer.eval()
    
    print(f"ğŸ“· åŠ è½½å†…å®¹å›¾: {content_path}")
    content_img = Image.open(content_path).convert("RGB")
    
    # ä¿å­˜åŸå§‹å°ºå¯¸ç”¨äºè¿˜åŸ
    original_size = content_img.size
    
    # é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor()
    ])
    
    content_tensor = transform(content_img).unsqueeze(0).to(DEVICE)
    
    # æ¨ç†
    print("âš¡ è¿›è¡Œé£æ ¼è¿ç§»...")
    with torch.no_grad():
        stylized_tensor = transformer(content_tensor)
    
    # åå¤„ç†ï¼ˆå‡å®šè¾“å‡ºåœ¨ [0,1]ï¼‰
    stylized_tensor = stylized_tensor.squeeze(0).cpu().clamp(0.0, 1.0)
    stylized_img = transforms.ToPILImage()(stylized_tensor)
    
    # è¿˜åŸåˆ°åŸå§‹å°ºå¯¸
    stylized_img = stylized_img.resize(original_size, Image.LANCZOS)
    
    # ä¿å­˜
    stylized_img.save(output_path)
    print(f"âœ… é£æ ¼è¿ç§»å®Œæˆï¼ä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æ¨ç†æ¨¡å¼ï¼šæ”¯æŒå¤šç§å‚æ•°æ ¼å¼
        # 1) python realtimeNST.py test                -> ä½¿ç”¨é»˜è®¤æ¨¡å‹å’Œ TEST_CONTENT_PATH
        # 2) python realtimeNST.py test <content>      -> ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼Œcontent å¯ä»¥æ˜¯ id æˆ–æ–‡ä»¶å
        # 3) python realtimeNST.py test <model> <content> -> æŒ‡å®šæ¨¡å‹æ–‡ä»¶ï¼ˆæœ¬ç›®å½•æˆ–ç»å¯¹è·¯å¾„ï¼‰å’Œ content
        args = sys.argv[2:]
        model_file = None
        content_arg = None

        if len(args) == 0:
            # ä½¿ç”¨é»˜è®¤
            model_file = MODEL_PATH
            content_arg = None
        elif len(args) == 1:
            a = args[0]
            if a.lower().endswith('.pth'):
                model_file = os.path.join(ROOT_DIR, a) if not os.path.isabs(a) else a
                content_arg = None
            else:
                model_file = MODEL_PATH
                content_arg = a
        else:
            # ä¸¤ä¸ªåŠä»¥ä¸Šå‚æ•°ï¼Œç¬¬ä¸€ä¸ºæ¨¡å‹ï¼Œç¬¬äºŒä¸ºå†…å®¹æ ‡è¯†
            m = args[0]
            content_arg = args[1]

            # æ”¯æŒçœç•¥ .pth åç¼€å¹¶åœ¨å½“å‰ç›®å½•æˆ– checkpoints ä¸­æŸ¥æ‰¾
            candidates = []
            if os.path.isabs(m):
                candidates.append(m)
                candidates.append(m + '.pth')
            else:
                candidates.append(os.path.join(ROOT_DIR, m))
                candidates.append(os.path.join(ROOT_DIR, m + '.pth'))
                candidates.append(os.path.join(CHECKPOINT_DIR, m))
                candidates.append(os.path.join(CHECKPOINT_DIR, m + '.pth'))

            model_file = None
            for c in candidates:
                if os.path.exists(c):
                    model_file = c
                    break
            if model_file is None:
                # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œä¿ç•™æœ€å¯èƒ½çš„è·¯å¾„ä»¥ä¾¿åç»­æŠ¥é”™å¹¶æç¤ºå¯ç”¨æ–‡ä»¶
                model_file = os.path.join(ROOT_DIR, m)
                print(f"âš ï¸ æœªæ‰¾åˆ°æŒ‡å®šæ¨¡å‹çš„å€™é€‰è·¯å¾„ï¼Œå·²å°è¯•: {candidates}")

        # éªŒè¯æ¨¡å‹æ–‡ä»¶
        if not os.path.exists(model_file):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
            # åˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„å¯ç”¨ pth æ–‡ä»¶å’Œ checkpoints
            available = [f for f in os.listdir(ROOT_DIR) if f.lower().endswith('.pth')]
            ck = []
            if os.path.exists(CHECKPOINT_DIR):
                ck = [f for f in os.listdir(CHECKPOINT_DIR) if f.lower().endswith('.pth')]
            if available or ck:
                print("å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ï¼š")
                for f in available[:20]:
                    print(f"  - {f}")
                for f in ck[:20]:
                    print(f"  - {os.path.join('checkpoints', f)}")
            else:
                print("åœ¨é¡¹ç›®ç›®å½•å’Œ checkpoints ä¸­æœªæ‰¾åˆ° .pth æ¨¡å‹æ–‡ä»¶")
            sys.exit(1)

        # ç¡®å®šå†…å®¹å›¾ç‰‡è·¯å¾„
        if content_arg:
            ca = content_arg
            if ca.lower().endswith(('.png', '.jpg', '.jpeg')):
                candidate = os.path.join(TEST_CONTENTS_DIR, ca)
            else:
                candidate = os.path.join(TEST_CONTENTS_DIR, f"testcontent-{ca}.png")
            content_path = candidate
        else:
            content_path = TEST_CONTENT_PATH

        if not os.path.exists(content_path):
            print(f"âŒ æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {content_path}")
            if os.path.exists(TEST_CONTENTS_DIR):
                files = [f for f in os.listdir(TEST_CONTENTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                print(f"å¯ç”¨çš„æµ‹è¯•å›¾ç‰‡ï¼ˆå­˜äº {TEST_CONTENTS_DIR}ï¼‰ï¼š")
                for f in files[:50]:
                    print(f"  - {f}")
            else:
                print(f"ç›®å½•ä¸å­˜åœ¨: {TEST_CONTENTS_DIR}")
            sys.exit(1)

        # è¾“å‡ºæ–‡ä»¶åç”Ÿæˆï¼šåŒ…å«æ¨¡å‹åï¼ˆæ— åç¼€ï¼‰å’Œå†…å®¹æ ‡è¯†
        model_base = os.path.splitext(os.path.basename(model_file))[0]
        if content_arg:
            content_id = os.path.splitext(os.path.basename(content_path))[0]
            # å»æ‰ testcontent- å‰ç¼€
            if content_id.startswith('testcontent-'):
                content_id = content_id[len('testcontent-'):]
            out_name = f"output_stylized-{model_base}-{content_id}.png"
        else:
            out_name = f"output_stylized-{model_base}.png"
        output_path = os.path.join(ROOT_DIR, out_name)

        stylize_image(content_path, model_file, output_path)
    elif len(sys.argv) > 1 and sys.argv[1] == "train":
        # è®­ç»ƒæ¨¡å¼ï¼šæ”¯æŒ `python realtimeNST.py train style1`
        style_arg = None
        if len(sys.argv) > 2:
            style_arg = sys.argv[2]
        main(style_arg)
    else:
        # è®­ç»ƒæ¨¡å¼
        main()
